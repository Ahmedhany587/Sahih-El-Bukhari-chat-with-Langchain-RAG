# -*- coding: utf-8 -*-
"""Hadith.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IwMkUSZodpU4YrdVU9NVX38nYzb7X9op

# Importing dependecies
"""

!pip install transformers torch  langchain langchain_community chainlit  sentence_transformers faiss-gpu  bitsandbytes accelerate --quiet

"""#Langchain"""

from langchain_community.document_loaders import DirectoryLoader
import pandas as pd
import faiss
from langchain.document_loaders import DataFrameLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS, Chroma
from langchain_community.embeddings import HuggingFaceHubEmbeddings
import numpy as np



from langchain_community.document_loaders.csv_loader import CSVLoader


data = pd.read_csv("/content/Sahih Bukhari Without_Tashkel.csv")
data.head()

loader = DataFrameLoader(data, page_content_column="Sahih Bukhari Without_Tashkel")
documents = loader.load()

documents[1]

"""Embedding Text"""

from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings
embedding_function = SentenceTransformerEmbeddings(model_name="intfloat/multilingual-e5-large")

store = FAISS.from_documents(documents, embedding_function)

query = """
ما هي نسبة الزكاة من المال

"""

result = store.similarity_search(query, k=10)


# Iterate over each document in the results
for i, doc in enumerate(result, start=1):
    # Print the document number
    print(f"Result {i}:")
    # Print the document content
    print(doc.page_content)
    # Optionally, print a separator line for readability
    print("-" * 100)

"""#RAG"""

from langchain.prompts import ChatPromptTemplate


prompt = """
استنادًا إلى المعارف والأحاديث الواردة في صحيح البخاري، الكتاب الشامل الذي يضم تعاليم النبي محمد ﷺ
ويعتبر أحد أهم مراجع الحديث في الإسلام، يُرجى تقديم إجابة مفصلة ودقيقة تعكس الفهم العميق للسؤال التالي
باللغة العربية
CONTEXT:
{context}

QUESTION:
{question}
"""
prompt = ChatPromptTemplate.from_template(prompt)

print(prompt)

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
#load_in_4bit=True,
load_in_4bit=True,
bnb_4bit_use_double_quant=True,
bnb_4bit_quant_type="nf4",
bnb_4bit_compute_dtype=torch.bfloat16
)

!huggingface-cli login

from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch

from transformers import pipeline


model_id ="meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map='cuda')
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512)
hf = HuggingFacePipeline(pipeline=pipe)

from langchain.chains import RetrievalQA
from langchain.chains import RetrievalQA



def retrieval_qa_chain(llm, prompt, db):
    qa_chain = RetrievalQA.from_chain_type(llm=hf,
                                       chain_type='stuff',
                                       retriever=db.as_retriever(search_kwargs={'k': 2}),
                                       return_source_documents=True,
                                       chain_type_kwargs={'prompt': prompt}
                                       )
    return qa_chain


qa = retrieval_qa_chain(hf, prompt, store)

def qa_bot():
    embeddings = embedding_function
    db = store
    qa_prompt = prompt
    qa = retrieval_qa_chain(hf, qa_prompt, store)
    return qa

def final_result(query):
    qa_result = qa_bot()
    response = qa_result({'query': query})
    return response

x1 = final_result("ما هو فضل ليلة القدر")
print(x1["query"])
print("="*20)
print(x1["result"])
print("="*20)
print(x1["source_documents"])

x1 = final_result("ما هي عقوبة تارك الصلاة")

print(x1["query"])
print("="*200)
print(x1["result"])
print("="*200)
print(x1["source_documents"])

x1 = final_result("من يدخل الجنة بغير حساب")
print(x1["query"])
print("="*20)
print(x1["result"])
print("="*20)
print(x1["source_documents"])

